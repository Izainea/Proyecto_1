{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db660058",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación inicial**\n",
    "\n",
    "El término ensamblador significa grupo. Los métodos tipo ensamblador están formados de un grupo de modelos predictivos que permiten alcanzar una mejor precisión y estabilidad del modelo. Estos proveen una mejora significativa a los modelos de árboles de decisión.\n",
    "\n",
    "**¿Por qué surgen los ensambladores de árboles?**\n",
    "\n",
    "* Así como todos los modelos, un árbol de decisión también sufre de los problemas de sesgo y varianza. Es decir, ‘cuánto en promedio son los valores predecidos diferentes de los valores reales’ (sesgo) y ‘cuan diferentes serán las predicciones de un modelo en un mismo punto si muestras diferentes se tomaran de la misma población’ (varianza).\n",
    "\n",
    "* Al construir un árbol pequeño se obtendrá un modelo con baja varianza y alto sesgo. Normalmente, al incrementar la complejidad del modelo, se verá una reducción en el error de predicción debido a un sesgo más bajo en el modelo. En un punto el modelo será muy complejo y se producirá un sobre-ajuste del modelo el cual empezará a sufrir de varianza alta.\n",
    "\n",
    "* El modelo óptimo debería mantener un balance entre estos dos tipos de errores. A esto se le conoce como “trade-off” (equilibrio) entre errores de sesgo y varianza. El uso de ensambladores es una forma de aplicar este “trade-off”.\n",
    "\n",
    "![tradeoff](https://bookdown.org/content/2031/images/trade_off.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensambladores comunes: Bagging, Boosting and Stacking. Random Forest es del primer tipo.\n",
    "\n",
    "¿Qué es el proceso de bagging y cómo funciona?\n",
    "\n",
    "Bagging es una técnica usada para reducir la varianza de las predicciones a través de la combinación de los resultados de varios clasificadores, cada uno de ellos modelados con diferentes subconjuntos tomados de la misma población.\n",
    "\n",
    "![bagging](https://bookdown.org/content/2031/images/bootstraping.png)\n",
    "\n",
    "La técnica de Bagging sigue estos pasos:\n",
    "\n",
    "1. Divide el set de Entrenamiento en distintos sub set de datos, obteniendo como resultado diferentes muestras aleatorias con las siguientes características:\n",
    "    * Muestra uniforme (misma cantidad de individuos en cada set)\n",
    "\n",
    "    * Muestras con reemplazo (los individuos pueden repetirse en el mismo set de datos).\n",
    "\n",
    "2. El tamaño de la muestra es igual al tamaño del set de entrenamiento, pero no contiene a todos los individuos ya que algunos se repiten.\n",
    "\n",
    "3. Si se usan muestras sin reemplazo, suele elegirse el 50% de los datos como tamaño de muestra\n",
    "\n",
    "4. Luego se crea un modelo predictivo con cada set, obteniendo modelos diferentes\n",
    "\n",
    "5. Luego se construye o ensambla un único modelo predictivo, que es el promedio de todos los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el Bagging hay que tener presente (http://apuntes-r.blogspot.com.es/search/label/Bagging):\n",
    "\n",
    "• Disminuye la varianza de un data set al realizar remuestreo con reemplazo..\n",
    "\n",
    "• Si no existe varianza en el data set, la tecnica de Baggin no mejora significativamente el modelo.\n",
    "\n",
    "• Es recomendable en modelos de alta inestabilidad (data set con mucha varianza). Ejemplo de inestabilidad: el % de error de la predicción de fraudes de enero , es muy diferente al de febrero.\n",
    "\n",
    "• Mientras más inestable es un modelo, mejor será la predicción al usar Bagging.\n",
    "\n",
    "• Se reduce el overfetting o sobre entrenamiento de modelos. Esto porque los modelos no pueden sobreaprender o memorizar ya que ninguno tiene todos los datos de entrenamiento.\n",
    "\n",
    "• Mejora la predicción, ya que lo que no detecta un modelo lo detectan los otros.\n",
    "\n",
    "• Reduce el ruido de los outliers, ya los outliers no pueden estar presenten en todos los modelos.\n",
    "\n",
    "• No mejora significativamente las funciones lineales, ya que el ensamble de una función lineal da como resultado otra función linear.\n",
    "\n",
    "• Una técnica mejorada del Bagging es el Random Forest que ademas de elegir un grupo aleatorio de individuos, también elige un grupo aleatorio de variables.\n",
    "\n",
    "• Los diferentes modelos creados con la técnica Bagging pueden considerarse como algoritmos que buscan respuestas (o hipótesis) en un data set (o espacio h). Como cada algoritmo tiene un set de datos diferentes, cada uno creará una hipótesis diferentes sobre la realidad.\n",
    "\n",
    "Realizamos un bagging para un clasificador binario, utilizando un scrip obtendido en: http://apuntes-r.blogspot.com.es/2015/01/bagging-para-clasificador-binario.html#more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Preprocesado y modelado\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n",
      "File \u001b[1;32mc:\\Users\\cizai\\miniconda3\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:157\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    109\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39m\n\u001b[0;32m    111\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124m        <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos\n",
    "\n",
    "\n",
    "El set de datos Boston disponible en la librería scikitlearn contiene precios de viviendas de la ciudad de Boston, así como información socio-económica del barrio en el que se encuentran. Se pretende ajustar un modelo de regresión que permita predecir el precio medio de una vivienda (MEDV) en función de las variables disponibles.\n",
    "\n",
    "Number of Instances: 506\n",
    "\n",
    "Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
    "\n",
    "Attribute Information (in order):\n",
    "\n",
    "* CRIM: per capita crime rate by town\n",
    "* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS: proportion of non-retail business acres per town\n",
    "* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX: nitric oxides concentration (parts per 10 million)\n",
    "* RM: average number of rooms per dwelling\n",
    "* AGE: proportion of owner-occupied units built prior to 1940\n",
    "* DIS: weighted distances to five Boston employment centres\n",
    "* RAD: index of accessibility to radial highways\n",
    "* TAX: full-value property-tax rate per USD 10,000\n",
    "* PTRATIO: pupil-teacher ratio by town\n",
    "* B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT: % lower status of the population\n",
    "* MEDV: Median value of owner-occupied homes in USD1000's\n",
    "* Missing Attribute Values: None\n",
    "\n",
    "Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se unen todos los datos (predictores y variable respuesta en un único dataframe)\n",
    "boston = load_boston(return_X_y=False)\n",
    "datos = np.column_stack((boston.data, boston.target))\n",
    "datos = pd.DataFrame(datos,columns = np.append(boston.feature_names, \"MEDV\"))\n",
    "datos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de los datos en train y test\n",
    "# ==============================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        datos.drop(columns = \"MEDV\"),\n",
    "                                        datos['MEDV'],\n",
    "                                        random_state = 123\n",
    "                                    )\n",
    "# Creación del modelo\n",
    "# ==============================================================================\n",
    "modelo = RandomForestRegressor(\n",
    "            n_estimators = 10,\n",
    "            criterion    = 'mse',\n",
    "            max_depth    = None,\n",
    "            max_features = 'auto',\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 123\n",
    "         )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ==============================================================================\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo inicial\n",
    "# ==============================================================================\n",
    "predicciones = modelo.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de hiperparámetros\n",
    "\n",
    "\n",
    "El modelo inicial se ha entrenado utilizando 10 árboles (n_estimators=10) y manteniendo el resto de hiperparámetros con su valor por defecto. Al ser hiperparámetros, no se puede saber de antemano cuál es el valor más adecuado, la forma de identificarlos es mediante el uso de estrategias de validación, por ejemplo validación cruzada.\n",
    "\n",
    "Los modelos Random Forest tienen la ventaja de disponer del Out-of-Bag error, lo que permite obtener una estimación del error de test sin recurrir a la validación cruzada, que es computacionalmente costosa. En la implementación de RandomForestRegressor, la métrica devuelta como oob_score es el  R2 , si se desea otra, sí se tiene que recurrir a validación cruzada. A continuación, se muestran las dos aproximaciones.\n",
    "\n",
    "Cabe tener en cuenta que, cuando se busca el valor óptimo de un hiperparámetro con dos métricas distintas, el resultado obtenido raramente es el mismo. Lo importante es que ambas métricas identifiquen las mismas regiones de interés.\n",
    "\n",
    "## Número de árboles\n",
    "\n",
    "\n",
    "En Random Forest, el número de árboles no es un hiperparámetro crítico en cuanto que, añadir árboles, solo puede hacer que mejorar el resultado. En Random Forest no se produce overfitting por exceso de árboles. Sin embargo, añadir árboles una vez que la mejora se estabiliza es una perdida te recursos computacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "estimator_range = range(1, 150, 5)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for n_estimators in estimator_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = n_estimators,\n",
    "                criterion    = 'mse',\n",
    "                max_depth    = None,\n",
    "                max_features = 'auto',\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 123\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "    \n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(estimator_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\", label=\"max score\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolución del out-of-bag-error vs número árboles\")\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "cv_scores    = []\n",
    "\n",
    "# Valores evaluados\n",
    "estimator_range = range(1, 150, 5)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# de entrenamiento y de k-cross-validation.\n",
    "for n_estimators in estimator_range:\n",
    "    \n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = n_estimators,\n",
    "                criterion    = 'mse',\n",
    "                max_depth    = None,\n",
    "                max_features = 'auto',\n",
    "                oob_score    = False,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 123\n",
    "             )\n",
    "    \n",
    "    # Error de train\n",
    "    modelo.fit(X_train, y_train)\n",
    "    predicciones = modelo.predict(X = X_train)\n",
    "    rmse = mean_squared_error(\n",
    "            y_true  = y_train,\n",
    "            y_pred  = predicciones,\n",
    "            squared = False\n",
    "           )\n",
    "    train_scores.append(rmse)\n",
    "    \n",
    "    # Error de validación cruzada\n",
    "    scores = cross_val_score(\n",
    "                estimator = modelo,\n",
    "                X         = X_train,\n",
    "                y         = y_train,\n",
    "                scoring   = 'neg_root_mean_squared_error',\n",
    "                cv        = 5\n",
    "             )\n",
    "    # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "    cv_scores.append(-1*scores.mean())\n",
    "    \n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, cv_scores, label=\"cv scores\")\n",
    "ax.plot(estimator_range[np.argmin(cv_scores)], min(cv_scores),\n",
    "        marker='o', color = \"red\", label=\"min score\")\n",
    "ax.set_ylabel(\"root_mean_squared_error\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolución del cv-error vs número árboles\")\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmin(cv_scores)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
